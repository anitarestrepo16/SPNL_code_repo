---
title: 'Stat 224: Homework 6'
author: "Anita Restrepo"
date: "11/20/2020"
fontsize: 11pt
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
header-includes: \usepackage{pdfpages}
geometry: margin=0.75in
fig_crop: no
---

```{r setup, include=FALSE}
library(MASS)
library(mosaic)
library(knitr)
library(broom)
library(tidyverse)
library(car)
options(width=70, digits=6, scipen=8, knitr.duplicate.label = "allow")
def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})
# Set R output size a bit smaller than default
opts_chunk$set(size='small') 
# Set the default to NOT display R code
opts_chunk$set(echo=TRUE) 
```
# Q1a
```{r}
brake = read.table("http://www.stat.uchicago.edu/~yibi/s224/data/brake.txt", header=T)
ggplot(brake, aes(x = speed, y = distance)) +
  geom_point()
```
```{r}
ols1 = lm(distance ~ speed + I(speed^2), data=brake)
ggplot(brake, aes(x = ols1$fitted.values, y = ols1$residuals)) +
  geom_point() +
  labs(title = "ols1 (no transformation)", x = "Fitted Values", y = "Raw Residuals") +
  geom_hline(yintercept = 0)
```
We see that there is heterogeneity of variance from both plots as the variance of distance increases with speed and the variance of the residuals increases with the fitted values.  
  
# Q1b
```{r}
ols2 = lm(sqrt(distance) ~ speed + I(speed^2), data=brake)
ols3 = lm(log(distance) ~ speed + I(speed^2), data=brake)
```
```{r, fig.show= "hold", out.width= "50%"}
ggplot(brake, aes(x = ols2$fitted.values, y = ols2$residuals)) +
  geom_point() +
  labs(title = "ols2 (square root transformation)", x = "Fitted Values", y = "Raw Residuals") +
  geom_hline(yintercept = 0)
ggplot(brake, aes(x = ols3$fitted.values, y = ols3$residuals)) +
  geom_point() +
  labs(title = "ols3 (log transformation)", x = "Fitted Values", y = "Raw Residuals") +
  geom_hline(yintercept = 0)
```
From these plots it is clear that the square root transform is better than both the log transform and no transformation because it completely erases the heterogeneity of variance, while the log transform seems to simply invert the direction.  
  
# Q1c  
```{r}
boxcox(ols1)
abline(v=1/2, col="red")
```
The 95% CI for the box cox method does not cover zero, meaning the log transform is NOT recommended. Instead, the closest interpretable value of $\lambda$ is 1/2 (though barely) as evidence by the abline.  
  
# Q1d
```{r}
predCI1 = predict(ols1, data.frame(speed = brake$speed), interval="prediction")
predCI2 = predict(ols2, data.frame(speed = brake$speed), interval="prediction")^2
predCI3 = exp(predict(ols3, data.frame(speed = brake$speed), interval="prediction"))
```

```{r, fig.show="hold", out.width="33%"}
ggplot(brake, aes(x=speed, y=distance)) +
  geom_ribbon(aes(ymin=predCI1[,2], ymax = predCI1[,3]), alpha=0.25) +
  geom_line(aes(y=predCI1[,1]), col="blue",lwd=1) +
  geom_point() + 
  labs(title = "ols1 (no transformation)")
ggplot(brake, aes(x=speed, y=distance)) +
  geom_ribbon(aes(ymin=predCI2[,2], ymax = predCI2[,3]), alpha=0.25) +
  geom_line(aes(y=predCI2[,1]), col="blue",lwd=1) +
  geom_point() + 
  labs(title = "ols2 (square root transformation)")
ggplot(brake, aes(x=speed, y=distance)) +
  geom_ribbon(aes(ymin=predCI3[,2], ymax = predCI3[,3]), alpha=0.25) +
  geom_line(aes(y=predCI3[,1]), col="blue",lwd=1) +
  geom_point() + 
  labs(title = "ols3 (log transformation)")
```
The prediction band for ols2 (quare root transform) best fits the data because it takes into account the changing variance of distance at different levels of speed (i.e. narrow at low speeds and wide at high speeds).  
  
# Q1e
```{r}
summary(ols2)
```
The square term is not significant even though the data seems to follow a polynomial relationship.
```{r}
ols4 = lm(sqrt(distance) ~ speed, data = brake)
summary(ols4)
```
```{r, fig.show="hold", out.width="50%"}
ggplot(brake, aes(x = ols4$fitted.values, y = rstudent(ols4))) +
  geom_point() +
  labs(title = "Residual Plot", x = "Fitted Values", y = "Studentized Residuals") +
  geom_hline(yintercept = 0)
qqnorm(rstudent(ols4), ylab="Studentized Residuals")
qqline(rstudent(ols4))
```
There doesn't seem to be heterogeneity of variance as the points seem to be scattered sufficiently randomly and evenly around zero in the residual plot. Additionally, the QQ plot does not seem to indicate any serious deviations from linearity and normality.  
  
# Q1f  
```{r}
predict(ols4, data.frame(speed = 10), interval="prediction")^2
```
```{r}
predict(ols4, data.frame(speed = 20), interval="prediction")^2
```
```{r}
predict(ols4, data.frame(speed = 30), interval="prediction")^2
```  
  
# Q2a  
```{r}
wls1 = lm(distance ~ speed + I(speed^2), data=brake, weight=1/speed)
wls2 = lm(distance ~ speed + I(speed^2), data=brake, weight=1/speed^2)
```
We should use the standardized residuals as opposed to the raw residuals because the raw residuals are not adjusted for the weights (Lecture 13, slide 14).
```{r, fig.show="hold", out.width="50%"}
ggplot(brake, aes(x = speed, y = rstandard(wls1))) +
  geom_point() +
  labs(title = "wls1 (weight = 1/speed)") +
  geom_hline(yintercept = 0)
ggplot(brake, aes(x = speed, y = rstandard(wls2))) +
  geom_point() +
  labs(title = "wls2 (weight = 1/speed^2)") +
  geom_hline(yintercept = 0)
```
It looks like wls2 is better because the points are more randomly and evenly scattered around the 0-line. In other words, the weight is inversely proportional to the square of speed.  
  
# Q2b  
```{r}
predict(wls2, data.frame(speed = 10), weights = 1/10^2, interval="prediction")
```
```{r}
predict(wls2, data.frame(speed = 20), weights = 1/20^2, interval="prediction")
```
```{r}
predict(wls2, data.frame(speed = 30), weights = 1/30^2, interval="prediction")
```  
  
# Q3a  
```{r}
fatherson = read.table("http://www.stat.uchicago.edu/~yibi/s224/data/fatherson.txt", h=T)
lm1 = lm(Aver.Height.Son ~ Height.Father, data=fatherson)
```
```{r, fig.show="hold", out.width="33%"}
ggplot(fatherson, aes(y = Height.Father, x = No.Fathers)) +
  geom_point() +
  labs(title = "Father Height by # of Fathers in Cluster", x = "# of Fathers in Cluster",
       y = "Father Height")
ggplot(fatherson, aes(x = No.Fathers, y = Aver.Height.Son)) +
  geom_point() +
  labs(title = "Average Son Height by # of Fathers in Cluster", x = "# of Fathers in Cluster",
       y = "Average Son Height")
ggplot(fatherson, aes(y = lm1$residuals, x = No.Fathers)) +
  geom_point() +
  labs(title = "lm1 Raw Residuals by # of Fathers in Cluster", x = "# of Fathers in Cluster",
       y = "lm1 Raw Residuals") +
  geom_hline(yintercept = 0)
```
It's clear this simple linear model is not appropriate because of the uneven number of fathers in each cluster and the clear dependence of both father and son heights on number of fathers.  
  
# Q3b
```{r}
wls = lm(Aver.Height.Son ~ Height.Father, data=fatherson, weight=No.Fathers)
ggplot(fatherson, aes(x = No.Fathers, y = rstandard(wls))) +
  geom_point() + geom_hline(yintercept = 0)
```
The weights should be the sample size in each cluster (i.e. number of fathers in each group). Based on the residual plot using standardized residuals (vs. raw residuals because those don't take the weights into account) plotted against the number of fathers in each group, it seems the heteroscedasticity has been taken care of by the WLS model.
```{r}
wls$coefficients
```
The prediction equation is: $$ \hat{y} = 32.58 + 0.53x + \epsilon$$  
  
# Q3c  
```{r}
predict(wls, data.frame(Height.Father = 70), weights = 1, interval="confidence")
```  
  
# Q4a  
```{r}
stock = read.table("http://www.stat.uchicago.edu/~yibi/s224/data/P229-30.txt", header=T)
ggplot(stock, aes(x = Day, y = DJIA)) + 
  geom_point()
```
DJIA overall increases with time in 1996 but there is definitely a systematic relationship between the DJIA on a certain day and the DJIA on the days close to that day (both before and after), suggesting the presence of autocorrelation.   
  
# Q4b  
  
## Residual Time Plot
```{r}
stock1 = lm(DJIA ~ Day, data=stock)
```
```{r}
ggplot(stock, aes(x = Day, y = stock1$residuals)) + 
  geom_point() +
  geom_line() +
  geom_hline(yintercept = 0)
```
There's definitely a clear clustering of contiguous residuals.  
  
## Runs Test
```{r}
number_neg = sum(stock1$residuals < 0)
number_pos = sum(stock1$residuals > 0)
df = tibble(residual = stock1$residuals, sign = sign(residual))
n_runs = length(rle(df$sign)$lengths)
```
$$ \mu = \frac{2n_1n_2}{n_1 + n_2}+1$$
```{r}
mu = 2*number_neg*number_pos/(number_neg+number_pos)+1
```
$$\sigma = \sqrt{\frac{2n_1n_2(2n_1n_2-n_1-n_2)}{(n_1+n_2)^2(n_1+n_2-1)}}$$
```{r}
sigma = sqrt(2*number_neg*number_pos*(2*number_neg*number_pos-number_neg-number_pos)/
       ((number_pos + number_neg)^2*(number_pos+number_neg-1)))
```
$$z = \frac{N_{runs} - \mu}{\sigma}$$
```{r}
z = (n_runs-mu)/sigma
2*pnorm(z)
```  
The runs test is very significant, indicating autocorrelation.  
  
## Durbin-Watson Test  
```{r}
durbinWatsonTest(stock1, alternative = "positive")
```  
The one-tailed Durbin Watson Test for positive autocorrelation is also significant, indicating positive autocorrelation.  
  
## Autocorrelation Plot  
```{r}
acf(stock1$residuals)
```
Literally all of the lags are beyond the limits and there is a decreasing pattern, once more indicating autocorrelation.  
  
# Q4c  
```{r}
lag.plot(stock$DJIA, lags=1)
```
There seems to be a very strong linear relationship between the value of DJIA at any time t and the value of DJIA at that time t-1. More evidence for autocorrelation, this time specifically at the lag-1 level.  
  
# Q4d  
```{r}
stock2 = lm(DJIA[2:262] ~ DJIA[1:261], data=stock)
```
  
## Residual Time Plot
```{r}
ggplot(stock[1:261,], aes(x = Day, y = stock2$residuals)) + 
  geom_point() +
  geom_line() +
  geom_hline(yintercept = 0)
```
The clear clustering of contiguous residuals seems to have disappeared.  
  
## Runs Test
```{r}
number_neg = sum(stock2$residuals < 0)
number_pos = sum(stock2$residuals > 0)
df = tibble(residual = stock2$residuals, sign = sign(residual))
n_runs = length(rle(df$sign)$lengths)
```
$$ \mu = \frac{2n_1n_2}{n_1 + n_2}+1$$
```{r}
mu = 2*number_neg*number_pos/(number_neg+number_pos)+1
```
$$\sigma = \sqrt{\frac{2n_1n_2(2n_1n_2-n_1-n_2)}{(n_1+n_2)^2(n_1+n_2-1)}}$$
```{r}
sigma = sqrt(2*number_neg*number_pos*(2*number_neg*number_pos-number_neg-number_pos)/
       ((number_pos + number_neg)^2*(number_pos+number_neg-1)))
```
$$z = \frac{N_{runs} - \mu}{\sigma}$$
```{r}
z = (n_runs-mu)/sigma
2*pnorm(z)
```  
The runs test is no longer significant, indicating the autocorrelation has disappeared.  
  
## Durbin-Watson Test  
```{r}
durbinWatsonTest(stock2, alternative = "positive")
```  
The one-tailed Durbin Watson Test for positive autocorrelation is still significant at the .05 level, though the actual p-value increased. Given the other evidence from the other methods, it is clear the autocorrelation has sufficiently disappeared, in spite of what Durbin-Watson might say.  
  
## Autocorrelation Plot  
```{r}
acf(stock2$residuals)
```
None of the lags go beyond the limits (even though k = 1 and k = 6 do come close). We fixed it!

